diff --git a/DataUtils.py b/DataUtils.py
index a267513..7865020 100644
--- a/DataUtils.py
+++ b/DataUtils.py
@@ -8,6 +8,7 @@ def plot_series(data, title=''):
     sns.displot(data, kind='kde')
     plt.subplots_adjust(top=.90)
     plt.title(title, y=1.04)
+    plt.savefig('plots/' + title + '.png')
     plt.show()
 
 
diff --git a/TrainLSTM.py b/TrainLSTM.py
index edd7160..aee6877 100644
--- a/TrainLSTM.py
+++ b/TrainLSTM.py
@@ -13,12 +13,15 @@ from datetime import datetime
 
 from evaluate import evaluate_model
 from pathlib import Path
+import wandb
 
+
+wandb.init(project="LSTM Training")
 # will train on GPU CUDA cores if they are available in the system
 device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
 
 # Hyper-parameters
-input_size = 20
+wandb.config.input_size = 20
 num_lstm_units = 60
 num_layers = 2
 output_size = 1
@@ -29,11 +32,11 @@ learning_rate = 0.01
 do_train = True
 evaluate = True
 
-use_onehot = False
-scaled = True
+use_onehot = True
+scaled = False
 use_min = False
 
-model_name = 'LSTM_embed_scaled_' + datetime.now().strftime("%Y%m%d-%H%M%S")
+model_name = 'LSTM_onehot_' + datetime.now().strftime("%Y%m%d-%H%M%S")
 
 # model_name = 'Onehot_no_scaling'
 # model_name = 'LSTM_Onehot_scaling'
@@ -47,7 +50,7 @@ def get_param_string():
         rt_unit = 'min'
 
     pstring = 'Onehot: ' + str(use_onehot) + \
-              ' input_size: ' + str(input_size) + \
+              ' input_size: ' + str(wandb.config.input_size) + \
               ', num_lstm_units: ' + str(num_lstm_units) + \
               ', num_layers: ' + str(num_layers) + ',\n' + \
               'output_size: ' + str(output_size) + \
@@ -65,18 +68,20 @@ aas = '-ACDEFGHIKLMNPQRSTVWY'
 # define the mappings for char to int and int to char
 vocab = dict((a, i) for i, a in enumerate(aas))
 to_int = SeqToInt(vocab)
-data_frame = pd.read_csv('data\\dia.txt', sep='\t')[['sequence', 'RT']]
+
+target_name = 'RT'
+data_frame = pd.read_csv('data\\2021-03-12-easypqp-frac-lib-openswath_processed.tsv', sep='\t')[['sequence', target_name]]
 if use_min:
-    data_frame['RT'] = data_frame['RT']/60
+    data_frame[target_name] = data_frame[target_name]/60
 
-rt_data = SequenceDataset(data_frame, 'RT', transform=to_int)
+rt_data = SequenceDataset(data_frame, target_name, transform=to_int)
 
 scaler = None
 if scaled:
     scaler = MinMaxScaler(feature_range=(-1, 1))
     rt_data.scale_targets(scaler)
 
-# DataUtils.plot_series(rt_data.get_targets(), 'RT distribution')
+DataUtils.plot_series(rt_data.get_targets(), 'RT distribution')
 
 num_data_points = len(rt_data)
 
@@ -89,9 +94,9 @@ train_dataset, test_dataset, valid_dataset = torch.utils.data.random_split(rt_da
                                                                            generator=torch.Generator().manual_seed(42))
 
 if use_onehot:
-    model = RTLSTMOnehot(input_size, num_lstm_units, num_layers, batch_size, vocab, device).to(device)
+    model = RTLSTMOnehot(wandb.config.input_size, num_lstm_units, num_layers, batch_size, vocab, device).to(device)
 else:
-    model = RTLSTM(input_size, num_lstm_units, num_layers, batch_size, vocab, device).to(device)
+    model = RTLSTM(wandb.config.input_size, num_lstm_units, num_layers, batch_size, vocab, device).to(device)
 
 train_loader, test_loader, val_loader = DataUtils.setup_data_loaders(train_dataset, test_dataset, valid_dataset,
                                                                      batch_size, pad_sort_collate)
@@ -155,6 +160,7 @@ if do_train:
 
             if (i + 1) % 100 == 0:
                 losses.append(sum_loss / 100)
+                wandb.log({'loss': sum_loss / 100})
                 losses_unscaled.append(unscaled_sum_loss / 100)
                 print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'
                       .format(epoch + 1, num_epochs, i + 1, total_step, sum_loss / 100))
diff --git a/clean_data.py b/clean_data.py
new file mode 100644
index 0000000..efc4fef
--- /dev/null
+++ b/clean_data.py
@@ -0,0 +1,15 @@
+import pandas as pd
+
+data = pd.read_csv('data\\2021-03-12-easypqp-frac-lib-openswath_new.tsv', sep='\t')
+
+# filter out the decoys
+data = data[data['Decoy'] == 0]
+
+# select relevant columns and drop duplicate rows
+data = data[['PrecursorCharge', 'NormalizedRetentionTime', 'PeptideSequence', 'PrecursorIonMobility']]
+
+data.rename(columns={'NormalizedRetentionTime': 'RT', 'PeptideSequence': 'sequence', 'PrecursorIonMobility': 'IM'}, inplace=True)
+data.drop_duplicates(keep='first', inplace=True)
+data.reset_index(inplace=True, drop=True)
+
+data.to_csv('data\\2021-03-12-easypqp-frac-lib-openswath_processed.tsv', sep='\t', index=False)
diff --git a/evaluate.py b/evaluate.py
index 4323103..fe9df9b 100644
--- a/evaluate.py
+++ b/evaluate.py
@@ -50,3 +50,4 @@ def evaluate_model(model, model_name, scaled, scaler, device, batch_size, val_lo
                           columns=['Actual', 'Pred'])
         df.to_csv(filename, index=False)
         print('Average Loss: MAE {}, Med Abs Error {}'.format(sum_val_loss_abs / total, np.median(all_losses)))
+        Path('data/' + model_name + '_errors.txt').write_text(('Average Loss: MAE {}, Med Abs Error {}'.format(sum_val_loss_abs / total, np.median(all_losses))))
diff --git a/prediction_analysis.r b/prediction_analysis.r
index c326266..027d49e 100644
--- a/prediction_analysis.r
+++ b/prediction_analysis.r
@@ -5,7 +5,7 @@ source("config.r")
 
 #TODO: make text bigger
 #TODO: change the colour of the 95th percentile lines so that they are distinct
-val_data <- read.table(paste0(data_dir, "LSTM_embed_scaled_20210427-201629_validation_tar_pred.csv"), header=TRUE, sep=",")
+val_data <- read.table(paste0(data_dir, "LSTM_onehot_scaled_20210510-190056_validation_tar_pred.csv"), header=TRUE, sep=",")
 
 val_data$resid <- val_data$Actual - val_data$Pred
 
@@ -35,14 +35,14 @@ ggplot(val_data, aes(x=Actual, y=Pred)) +
   #geom_line(aes(y = Actual + bounds99CI["99.5%"]), size=0.5, colour = "black") +
   geom_line(aes(y = Actual), size=0.75, colour = "black") +
   geom_smooth(method = lm, se = FALSE, colour = "#1FCC00", size=0.75) +
-  #ylim(-5000,12000) +
-  #xlim(-5000,12000) +
+  ylim(0,120) +
+  xlim(0,120) +
   xlab("Observed RT (sec)") +
   ylab("Predicted RT (sec)") +
-  geom_text(x=8000, y=-300, label=paste("R^2:", r2, "\nInterval(sec):", interval, "\nInterval(min):", interval/60, sep=" "), size=4) +
+  geom_text(x=80, y=10, label=paste("R^2:", r2, "\nInterval(sec):", interval*60, "\nInterval(min):", interval, sep=" "), size=4) +
   theme_bw() +
   theme(plot.title = element_text(hjust = 0.5)) +
-  ggtitle("Actual vs Pred data. Validation Set. LSTM Embed.")
+  ggtitle("Actual vs Pred data. Validation Set. LSTM Onehot.")
 
 
-ggsave(paste0(plot_dir, "LSTM_embed_scaled_20210427-201629_validation_tar_pred.png"))
\ No newline at end of file
+ggsave(paste0(plot_dir, "LSTM_onehot_scaled_20210510-190056_validation_tar_pred.png"))
\ No newline at end of file
